{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OB1A2tI1OmOk",
        "outputId": "227bb271-03dd-4c5e-ab29-2db94fda6445"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textstat in /usr/local/lib/python3.10/dist-packages (0.7.3)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.10/dist-packages (from textstat) (0.14.0)\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.10/dist-packages (0.9.2)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.10/dist-packages (from fasttext) (2.12.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.25.2)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.3.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.22.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install textstat\n",
        "!pip install fasttext\n",
        "!pip install gensim\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "s5Vq4xBCOeWk",
        "outputId": "d25bd900-f6d7-4a4c-e964-81cc9de8cc30"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Package cmudict is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('cmudict')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fE7fnvsxFcvC"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import textstat\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def remove_possessive_markers(sentence):\n",
        "    if not isinstance(sentence, str):\n",
        "        sentence = str(sentence)\n",
        "    return re.sub(r\"'s|'\", \"\", sentence)\n",
        "\n",
        "def syllable_count(word):\n",
        "    return max(1, len(re.findall(r'[aeiouy]+', word, re.IGNORECASE)))\n",
        "\n",
        "def normalize_score(score, min_value, max_value):\n",
        "    return (score - min_value) / (max_value - min_value) if max_value != min_value else 0.5\n",
        "\n",
        "def calculate_sentence_complexity(sentence):\n",
        "    # Remove possessive markers\n",
        "    sentence = str(remove_possessive_markers(sentence))\n",
        "\n",
        "    # Tokenize the sentence\n",
        "    doc = nlp(sentence)\n",
        "    #tokens = nltk.word_tokenize(sentence)\n",
        "    tokens = [token for token in doc if not token.is_punct and not token.is_space\n",
        "              and not token.like_email and not token.like_url and not token.text.endswith('.')]\n",
        "\n",
        "    # Get unique lemmas\n",
        "    unique_lemmas = set(token.lemma_ for token in doc if token.text != '.')\n",
        "\n",
        "    # Calculate word metrics\n",
        "    word_metrics = [(token.text, syllable_count(token.text), 'complex' if syllable_count(token.text) >= 3 else 'simple') for token in tokens]\n",
        "\n",
        "    # Calculate total syllables, total words, and complex word count\n",
        "    total_syllables = sum(metric[1] for metric in word_metrics)\n",
        "    total_words = len(word_metrics)\n",
        "    complex_word_count = sum(1 for _, syllables, _ in word_metrics if syllables >= 3)\n",
        "\n",
        "    # Calculate metrics\n",
        "    total_complex_ratio_norm = complex_word_count / total_words if total_words != 0 else 0\n",
        "    max_dependency_depth = max(len(list(token.ancestors)) + 1 for token in doc)\n",
        "\n",
        "    # Normalize and calculate metrics\n",
        "    coleman_liau_index = normalize_score(textstat.coleman_liau_index(sentence), -8, 28)\n",
        "    gunning_fog_index = normalize_score(textstat.gunning_fog(sentence), 0, 32)\n",
        "    flesch_reading_ease_score = 1 - normalize_score(textstat.flesch_reading_ease(sentence), -73, 121)\n",
        "    dale_chall_readability_score = normalize_score(textstat.dale_chall_readability_score(sentence), 0, 20)\n",
        "    automated_readability_index = normalize_score(textstat.automated_readability_index(sentence), -5, 23)\n",
        "    vocabulary_complexity = len(unique_lemmas) / total_words if total_words != 0 else 0\n",
        "    sentence_length = total_words / 100\n",
        "    dependency_depth = normalize_score(max_dependency_depth, 1, 8)\n",
        "\n",
        "    # Calculate overall complexity score\n",
        "    overall_complexity_score = (\n",
        "        coleman_liau_index * 0.15 +\n",
        "        gunning_fog_index * 0.15 +\n",
        "        flesch_reading_ease_score * 0.15 +\n",
        "        dale_chall_readability_score * 0.15 +\n",
        "        automated_readability_index * 0.1 +\n",
        "        (len(set(token.text for token in doc)) / total_words if total_words != 0 else 0) * 0.1 +\n",
        "        total_complex_ratio_norm * 0.1 +\n",
        "        sentence_length * 0.05 +\n",
        "        dependency_depth * 0.05\n",
        "    )\n",
        "\n",
        "    # Return the numpy array of metric values\n",
        "    metric_values = np.array([\n",
        "        coleman_liau_index,\n",
        "        gunning_fog_index,\n",
        "        flesch_reading_ease_score,\n",
        "        dale_chall_readability_score,\n",
        "        automated_readability_index,\n",
        "        vocabulary_complexity,\n",
        "        total_complex_ratio_norm,\n",
        "        sentence_length,\n",
        "        dependency_depth,\n",
        "        overall_complexity_score\n",
        "    ])\n",
        "\n",
        "    return metric_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FlsHZgGdOwK3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYPWPXzWUPvU"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyKpFkCcOwA3"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2ZPy0lmUt1m"
      },
      "outputs": [],
      "source": [
        "glove_vectors = api.load('glove-wiki-gigaword-50')  # For 50-dimensional vectors\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load SQuAD dataset\n",
        "squad_dataset = load_dataset('squad')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kV-vdIEWKZh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import fasttext\n",
        "from nltk.tokenize import word_tokenize\n",
        "import torch\n",
        "\n",
        "# Prepare input features for the first example in the training set\n",
        "question = squad_dataset['train'][0]['question']\n",
        "context = squad_dataset['train'][0]['context']\n",
        "\n",
        "def get_word_embedding(word, glove_vectors):\n",
        "    try:\n",
        "        return glove_vectors[word]\n",
        "    except KeyError:\n",
        "        return np.zeros(glove_vectors.vector_size)\n",
        "\n",
        "def prepare_input_features(question, context, glove_vectors, embedding_dim, batch_size):\n",
        "    # Calculate metrics for question and context\n",
        "    question_metrics = calculate_sentence_complexity(question)\n",
        "    context_metrics = calculate_sentence_complexity(context)\n",
        "\n",
        "    # Tokenize questions and contexts\n",
        "    question_tokenized = word_tokenize(' '.join(question))\n",
        "    context_tokenized = word_tokenize(' '.join(context))\n",
        "\n",
        "    #print(\"Max Question Length:\", max_question_length)\n",
        "    #print(\"Max Context Length:\", max_context_length)\n",
        "\n",
        "    # Prepare input features for question\n",
        "    question_features = []\n",
        "    for start_index in range(0, len(question_tokenized), batch_size):\n",
        "        end_index = min(start_index + batch_size, len(question_tokenized))\n",
        "        batch_question_tokenized = question_tokenized[start_index:end_index]\n",
        "        batch_question_embeddings = []\n",
        "        for word in batch_question_tokenized:\n",
        "            embedding = get_word_embedding(word, glove_vectors)\n",
        "            combined_feature = np.concatenate([embedding, question_metrics])\n",
        "            batch_question_embeddings.append(combined_feature)\n",
        "\n",
        "        # Pad or truncate the batch_question_embeddings to the maximum length across the dataset\n",
        "        padded_batch_question_embeddings = np.zeros((max_question_length, combined_feature.shape[-1]))\n",
        "        for i, embedding in enumerate(batch_question_embeddings):\n",
        "            padded_batch_question_embeddings[i, :embedding.shape[0]] = embedding\n",
        "\n",
        "        #print(\"Shape of batch_question_embeddings before padding/truncation:\", np.array(batch_question_embeddings).shape)\n",
        "        #print(\"Shape of padded_batch_question_embeddings:\", padded_batch_question_embeddings.shape)\n",
        "\n",
        "        question_features.append(padded_batch_question_embeddings)\n",
        "\n",
        "    # Prepare input features for context\n",
        "    context_features = []\n",
        "    for start_index in range(0, len(context_tokenized), batch_size):\n",
        "        end_index = min(start_index + batch_size, len(context_tokenized))\n",
        "        batch_context_tokenized = context_tokenized[start_index:end_index]\n",
        "        batch_context_embeddings = []\n",
        "        for word in batch_context_tokenized:\n",
        "            embedding = get_word_embedding(word, glove_vectors)\n",
        "            combined_feature = np.concatenate([embedding, context_metrics])\n",
        "            batch_context_embeddings.append(combined_feature)\n",
        "\n",
        "        # Pad or truncate the batch_context_embeddings to the maximum length across the dataset\n",
        "        padded_batch_context_embeddings = np.zeros((max_context_length, combined_feature.shape[-1]))\n",
        "        for i, embedding in enumerate(batch_context_embeddings):\n",
        "            padded_batch_context_embeddings[i, :embedding.shape[0]] = embedding\n",
        "\n",
        "        #print(\"Shape of batch_context_embeddings before padding/truncation:\", np.array(batch_context_embeddings).shape)\n",
        "        #print(\"Shape of padded_batch_context_embeddings:\", padded_batch_context_embeddings.shape)\n",
        "\n",
        "        context_features.append(padded_batch_context_embeddings)\n",
        "\n",
        "    # Convert lists of numpy arrays to a single numpy array\n",
        "    question_features_array = np.concatenate(question_features, axis=0)\n",
        "    context_features_array = np.concatenate(context_features, axis=0)\n",
        "\n",
        "    #print(\"Length of question_features just before it's returned:\", len(question_features_array))\n",
        "    #print(\"Length of context_features just before it's returned:\", len(context_features_array))\n",
        "\n",
        "    # Convert the numpy arrays to PyTorch tensors\n",
        "    question_features_tensor = torch.tensor(question_features_array)\n",
        "    context_features_tensor = torch.tensor(context_features_array)\n",
        "\n",
        "    return {'question_features': question_features_tensor, 'context_features': context_features_tensor}\n",
        "\n",
        "# Preprocess the dataset\n",
        "batch_size = 8\n",
        "embedding_dim = glove_vectors.vector_size\n",
        "features = prepare_input_features(question, context, glove_vectors, embedding_dim, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pV5LkapCYWkK"
      },
      "outputs": [],
      "source": [
        "def find_max_lengths(dataset):\n",
        "    max_question_length = 0\n",
        "    max_context_length = 0\n",
        "    for example in dataset:\n",
        "        question_tokenized = word_tokenize(' '.join(example['question']))\n",
        "        context_tokenized = word_tokenize(' '.join(example['context']))\n",
        "        max_question_length = max(max_question_length, len(question_tokenized))\n",
        "        max_context_length = max(max_context_length, len(context_tokenized))\n",
        "    return max_question_length, max_context_length\n",
        "\n",
        "# Find maximum lengths for the train dataset\n",
        "train_dataset = squad_dataset['train']\n",
        "max_question_length, max_context_length = find_max_lengths(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mGrGNeJHeI-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l87WrA0beNE7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjVL4Zo2-UCF"
      },
      "outputs": [],
      "source": [
        "print(max_question_length)\n",
        "print(max_context_length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2w3GlT6wnV4T"
      },
      "outputs": [],
      "source": [
        "question_features_tensor = features['question_features']\n",
        "context_features_tensor = features['context_features']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y51G99KsnVvn"
      },
      "outputs": [],
      "source": [
        "print(\"Question features tensor shape:\", question_features_tensor.shape)\n",
        "print(\"Context features tensor shape:\", context_features_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgK8TDbxnfwq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "question_features_np = question_features_tensor.numpy()\n",
        "context_features_np = context_features_tensor.numpy()\n",
        "\n",
        "print(\"First few question features:\")\n",
        "print(question_features_np[:5])\n",
        "\n",
        "print(\"\\nFirst few context features:\")\n",
        "print(context_features_np[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfHs2v4V-V8o"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxLQ_lT5eV4A"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319,
          "referenced_widgets": [
            "b1cbd7191d7b4889abc580084a09048a",
            "cde97930cdda4e3c90902a6a88837dd2",
            "d5ab8980a3a349e1bd289cb8c8fcb5ed",
            "45bd97adda5a4b59b6bfaa9d2b591cd8",
            "b7e1ec1e577b46a094a5b28779ba4180",
            "6b7de68c08d74c81b0fc6959263c7636",
            "c2a3b6ad2a6c444199a86eaa6852de24",
            "9f8c834adbe24576b20f4cdcf078802f",
            "349433c0ab0f461083fc88ee5033fb69",
            "c832f729bfab4a8280a663109e8b3801",
            "2a59635c81664009a06f0b0178c74a30"
          ]
        },
        "id": "8w_lIByhTtnE",
        "outputId": "e52f03f8-e3a9-4f96-e65c-e2f6bb3bc331"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/10570 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1cbd7191d7b4889abc580084a09048a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of question_features: 1848\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ArrowInvalid",
          "evalue": "Column 4 named question_features expected length 1000 but got length 1848",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mArrowInvalid\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-185413b4e144>\u001b[0m in \u001b[0;36m<cell line: 126>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;31m# Load datasets and model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_preprocess_squad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0meval_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-185413b4e144>\u001b[0m in \u001b[0;36mload_preprocess_squad\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m   \u001b[0;31m# Select a random subset of 10000 samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m   \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msquad_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Seed for reproducibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m   \u001b[0meval_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msquad_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocess_squad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msquad_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Dataset\"\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"self\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m         }\n\u001b[1;32m    566\u001b[0m         \u001b[0;31m# apply actual function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 567\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"DatasetDict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    568\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Dataset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0;31m# re-apply format to the output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3154\u001b[0m                     \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesc\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"Map\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3155\u001b[0m                 ) as pbar:\n\u001b[0;32m-> 3156\u001b[0;31m                     \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_map_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mdataset_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3157\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3158\u001b[0m                             \u001b[0mshards_done\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3568\u001b[0m                                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_arrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3569\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3570\u001b[0;31m                                 \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3571\u001b[0m                         \u001b[0mnum_examples_progress_update\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum_examples_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3572\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_time\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPBAR_REFRESH_TIME_INTERVAL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_writer.py\u001b[0m in \u001b[0;36mwrite_batch\u001b[0;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[1;32m    569\u001b[0m                 \u001b[0minferred_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtyped_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_inferred_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minferred_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrow_schema\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpa_writer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         \u001b[0mpa_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    572\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table.from_arrays\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/table.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.Table.validate\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mArrowInvalid\u001b[0m: Column 4 named question_features expected length 1000 but got length 1848"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import RobertaTokenizer, RobertaForQuestionAnswering, RobertaConfig\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "class CustomEmbeddingLayer(nn.Module):\n",
        "    def __init__(self, output_dim):  # Change: Remove input_dim\n",
        "        super(CustomEmbeddingLayer, self).__init__()\n",
        "        self.projection = nn.Linear(self.get_input_dim(), output_dim)  # Change: Dynamic calculation\n",
        "\n",
        "    def get_input_dim(self): # New helper method\n",
        "        # Assuming your features are always the last dimension of the input\n",
        "        return self.current_input_shape[-1]\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.current_input_shape = x.shape  # Store the input shape\n",
        "        projected_embeddings = self.projection(x)\n",
        "        return projected_embeddings\n",
        "\n",
        "# Custom Question Answering Model\n",
        "class CustomQAModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(CustomQAModel, self).__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.qa_outputs = nn.Linear(self.hidden_size, 2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids, start_positions=None, end_positions=None, question_features=None, context_features=None):\n",
        "        # Use question_features and context_features directly as input\n",
        "        combined_question_input = question_features\n",
        "        combined_context_input = context_features\n",
        "\n",
        "        # Feed the combined input to the QA model\n",
        "        logits = self.qa_outputs(combined_input)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1).contiguous()\n",
        "        end_logits = end_logits.squeeze(-1).contiguous()\n",
        "\n",
        "        total_loss = None\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            ignored_index = start_logits.size(-1)\n",
        "            start_positions = start_positions.clamp(0, ignored_index)\n",
        "            end_positions = end_positions.clamp(0, ignored_index)\n",
        "\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)\n",
        "            start_loss = loss_fct(start_logits, start_positions)\n",
        "            end_loss = loss_fct(end_logits, end_positions)\n",
        "            total_loss = (start_loss + end_loss) / 2\n",
        "\n",
        "        return total_loss, start_logits, end_logits\n",
        "\n",
        "# Load and preprocess the SQuAD dataset\n",
        "def load_preprocess_squad():\n",
        "  squad_dataset = load_dataset('squad')\n",
        "  tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "  def preprocess_squad(examples):\n",
        "      questions = [q.strip() for q in examples['question']]\n",
        "      contexts = [c.strip() for c in examples['context']]\n",
        "      answers = examples['answers']\n",
        "\n",
        "      # Extract start positions and calculate end positions\n",
        "      start_positions = [answer['answer_start'][0] for answer in answers]  # Extracting the first element from the list\n",
        "      end_positions = [start + len(answer['text'][0]) for start, answer in zip(start_positions, answers)]  # Extracting the first element from the list\n",
        "\n",
        "      input_features = prepare_input_features(question, context, glove_vectors, embedding_dim, batch_size)\n",
        "\n",
        "      model_inputs = tokenizer(examples['question'], examples['context'], max_length=512, truncation=True, padding='max_length', return_tensors='pt')\n",
        "      output = {\n",
        "          'input_ids': model_inputs['input_ids'],\n",
        "          'attention_mask': model_inputs['attention_mask'],\n",
        "          'start_positions': start_positions,\n",
        "          'end_positions': end_positions,\n",
        "          'question_features': input_features['question_features'],\n",
        "          'context_features': input_features['context_features']\n",
        "      }\n",
        "\n",
        "      # Print the length of the question_features column\n",
        "      print(f\"Length of question_features: {len(output['question_features'])}\")\n",
        "\n",
        "      return output\n",
        "\n",
        "  # Select a random subset of 10000 samples\n",
        "  train_dataset = squad_dataset['train'].shuffle(seed=42).select(range(10000))  # Seed for reproducibility\n",
        "  eval_dataset = squad_dataset['validation'].map(preprocess_squad, batched=True, remove_columns=squad_dataset['validation'].column_names)\n",
        "\n",
        "  return train_dataset, eval_dataset\n",
        "\n",
        "# Training and evaluation functions\n",
        "def train_epoch(model, train_dataloader, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_dataloader, desc='Training'):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        loss, _, _ = model(**batch)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    return total_loss / len(train_dataloader)\n",
        "\n",
        "def eval_model(model, eval_dataloader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for batch in eval_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            loss, start_logits, end_logits = model(**batch)\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(eval_dataloader)\n",
        "\n",
        "# Main training loop\n",
        "def train_model(model, train_dataloader, eval_dataloader, optimizer, num_epochs=3):\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train_epoch(model, train_dataloader, optimizer)\n",
        "        val_loss = eval_model(model, eval_dataloader)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "# Load datasets and model\n",
        "batch_size = 8\n",
        "train_dataset, eval_dataset = load_preprocess_squad()\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "eval_dataloader = torch.utils.data.DataLoader(eval_dataset, batch_size=batch_size)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_config = RobertaConfig.from_pretrained('roberta-base')\n",
        "model = CustomQAModel.from_pretrained('roberta-base', config=model_config)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_dataloader, eval_dataloader, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWUCTj6sWUP-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbYGX59xwQrx",
        "outputId": "e463f7ea-51ee-4f45-bd66-f02032ef28de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n",
            "---\n",
            "{'text': ['a copper statue of Christ'], 'answer_start': [188]}\n",
            "---\n",
            "{'text': ['the Main Building'], 'answer_start': [279]}\n",
            "---\n",
            "{'text': ['a Marian place of prayer and reflection'], 'answer_start': [381]}\n",
            "---\n",
            "{'text': ['a golden statue of the Virgin Mary'], 'answer_start': [92]}\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "for example in squad_dataset['train'].select(range(5)):\n",
        "    print(example['answers'])\n",
        "    print('---')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O87hZfbyTtj3",
        "outputId": "36b721ba-8fbd-4880-ddd5-f16677cbdde5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['id', 'title', 'context', 'question', 'answers']\n"
          ]
        }
      ],
      "source": [
        "print(squad_dataset['train'].column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgJg-0JaoyDj"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NslTh7cwox6s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMYhdZc_oxxc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xcthBTgtQxI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ3QgBuntQtP"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gx21X2uQtQl0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSlAG0dWwjhM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8Vj5oF4wjdu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mm4_kDFkwjap"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2EZ4GZYZGyQ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import fasttext\n",
        "from nltk.tokenize import word_tokenize\n",
        "import torch\n",
        "\n",
        "# Prepare input features for the first example in the training set\n",
        "question = squad_dataset['train'][0]['question']\n",
        "context = squad_dataset['train'][0]['context']\n",
        "\n",
        "def get_word_embedding(word, glove_vectors):\n",
        "    try:\n",
        "        return glove_vectors[word]\n",
        "    except KeyError:\n",
        "        return np.zeros(glove_vectors.vector_size)\n",
        "\n",
        "def get_positional_encodings(sequence_length, embedding_dim):\n",
        "    pe = np.zeros((sequence_length, embedding_dim))\n",
        "    position = np.arange(0, sequence_length, dtype=np.float32)\n",
        "    div_term = np.exp(np.arange(0, embedding_dim, 2, dtype=np.float32) * (-np.log(10000.0) / embedding_dim))\n",
        "    position = position[:, np.newaxis]  # Convert to column vector to enable broadcasting\n",
        "    pe[:, 0::2] = np.sin(position * div_term)\n",
        "    pe[:, 1::2] = np.cos(position * div_term)\n",
        "    return pe\n",
        "\n",
        "def prepare_input_features(question, context, glove_vectors, embedding_dim, batch_size):\n",
        "    # Calculate metrics for question and context\n",
        "    question_metrics = calculate_sentence_complexity(question)\n",
        "    context_metrics = calculate_sentence_complexity(context)\n",
        "\n",
        "    # Tokenize questions and contexts\n",
        "    question_tokenized = word_tokenize(' '.join(question))\n",
        "    context_tokenized = word_tokenize(' '.join(context))\n",
        "\n",
        "    # Get maximum sequence lengths for questions and contexts\n",
        "    max_question_length = max(len(question_tokenized), 1)  # Ensure minimum length of 1\n",
        "    max_context_length = max(len(context_tokenized), 1)  # Ensure minimum length of 1\n",
        "\n",
        "    # Prepare input features for question\n",
        "    question_features = []\n",
        "    for start_index in range(0, len(question_tokenized), batch_size):\n",
        "        end_index = min(start_index + batch_size, len(question_tokenized))\n",
        "        batch_question_tokenized = question_tokenized[start_index:end_index]\n",
        "        batch_question_embeddings = []\n",
        "        for word in batch_question_tokenized:\n",
        "            embedding = get_word_embedding(word, glove_vectors)\n",
        "            positional_encoding = get_positional_encodings(max_question_length, embedding_dim)\n",
        "            combined_feature = np.concatenate([embedding, positional_encoding.flatten(), question_metrics])\n",
        "            batch_question_embeddings.append(combined_feature)\n",
        "        question_features.extend(batch_question_embeddings)\n",
        "\n",
        "    # Prepare input features for context\n",
        "    context_features = []\n",
        "    for start_index in range(0, len(context_tokenized), batch_size):\n",
        "        end_index = min(start_index + batch_size, len(context_tokenized))\n",
        "        batch_context_tokenized = context_tokenized[start_index:end_index]\n",
        "        batch_context_embeddings = []\n",
        "        for word in batch_context_tokenized:\n",
        "            embedding = get_word_embedding(word, glove_vectors)\n",
        "            positional_encoding = get_positional_encodings(max_question_length, embedding_dim)\n",
        "            combined_feature = np.concatenate([embedding, positional_encoding.flatten(), context_metrics])\n",
        "            batch_context_embeddings.append(combined_feature)\n",
        "        context_features.extend(batch_context_embeddings)\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    question_features_tensor = torch.tensor(question_features)\n",
        "    context_features_tensor = torch.tensor(context_features)\n",
        "\n",
        "    # Return input features as a dictionary\n",
        "    return {'question_features': question_features_tensor, 'context_features': context_features_tensor}\n",
        "\n",
        "embedding_dim = glove_vectors.vector_size\n",
        "batch_size = 100\n",
        "features = prepare_input_features(question, context, glove_vectors, embedding_dim, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ja3URLhVwjXd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S37anclwjUI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaNBVWBHXi9r",
        "outputId": "c1047c8f-33f9-4d74-9e01-3e0702450e5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentence Metrics: {'coleman_liau_index': 0.5763888888888888, 'gunning_fog_index': 0.3584375, 'flesch_reading_ease_score': 0.3616494845360825, 'dale_chall_readability_score': 0.475, 'automated_readability_index': 0.5392857142857143, 'vocabulary_complexity': 1.0, 'total_complex_word_ratio': 0.25, 'sentence_length': 0.12, 'dependency_depth': 0.42857142857142855, 'overall_complexity_score': 0.48041185720422186, 'complexity': 'Moderate', 'word_metrics': [('Consuming', 3, 'complex'), ('cannabis', 3, 'complex'), ('can', 1, 'simple'), ('lead', 1, 'simple'), ('a', 1, 'simple'), ('person', 2, 'simple'), ('to', 1, 'simple'), ('hallucinations', 5, 'complex'), ('and', 1, 'simple'), ('disturb', 2, 'simple'), ('one', 2, 'simple'), ('mind', 1, 'simple')]}\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import RobertaTokenizer, RobertaForQuestionAnswering, RobertaConfig\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Custom Embedding Layer\n",
        "class CustomEmbeddingLayer(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(CustomEmbeddingLayer, self).__init__()\n",
        "        self.projection = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        projected_embeddings = self.projection(x)\n",
        "        return projected_embeddings\n",
        "\n",
        "# Custom Question Answering Model\n",
        "class CustomQAModel(RobertaForQuestionAnswering):\n",
        "    def __init__(self, config):\n",
        "        super(CustomQAModel, self).__init__(config)\n",
        "        self.roberta = RobertaForQuestionAnswering(config)\n",
        "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
        "        self.custom_embedding = CustomEmbeddingLayer(input_dim=config.hidden_size + 11, output_dim=config.hidden_size)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, token_type_ids, start_positions=None, end_positions=None, question_features=None, context_features=None):\n",
        "        outputs = self.roberta(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        sequence_output = outputs[0]\n",
        "\n",
        "        # Concatenate token embeddings with question and context features\n",
        "        question_metrics_tensor = question_features.repeat(sequence_output.shape[1], 1).transpose(0, 1)\n",
        "        context_metrics_tensor = context_features.repeat(sequence_output.shape[1], 1).transpose(0, 1)\n",
        "        combined_question_input = torch.cat((sequence_output, question_metrics_tensor), dim=2)\n",
        "        combined_context_input = torch.cat((sequence_output, context_metrics_tensor), dim=2)\n",
        "\n",
        "        # Project combined inputs to the expected embedding dimension\n",
        "        combined_question_input = self.custom_embedding(combined_question_input)\n",
        "        combined_context_input = self.custom_embedding(combined_context_input)\n",
        "\n",
        "        # Feed the combined input to the QA model\n",
        "        logits = self.qa_outputs(combined_input)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1).contiguous()\n",
        "        end_logits = end_logits.squeeze(-1).contiguous()\n",
        "\n",
        "        total_loss = None\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            ignored_index = start_logits.size(-1)\n",
        "            start_positions = start_positions.clamp(0, ignored_index)\n",
        "            end_positions = end_positions.clamp(0, ignored_index)\n",
        "\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=ignored_index)\n",
        "            start_loss = loss_fct(start_logits, start_positions)\n",
        "            end_loss = loss_fct(end_logits, end_positions)\n",
        "            total_loss = (start_loss + end_loss) / 2\n",
        "\n",
        "        return total_loss, start_logits, end_logits\n",
        "\n",
        "# Load and preprocess the SQuAD dataset\n",
        "def load_preprocess_squad():\n",
        "  squad_dataset = load_dataset('squad')\n",
        "  tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "  def preprocess_squad(examples):\n",
        "    questions = [q.strip() for q in examples['question']]\n",
        "    contexts = [c.strip() for c in examples['context']]\n",
        "    input_features = prepare_input_features(questions, contexts, model)  # Call your function from the second program\n",
        "\n",
        "    model_inputs = tokenizer(examples['question'], examples['context'], max_length=512, truncation=True, padding='max_length', return_tensors='pt')\n",
        "    output = {\n",
        "        'input_ids': model_inputs['input_ids'],\n",
        "        'attention_mask': model_inputs['attention_mask'],\n",
        "        'token_type_ids': model_inputs['token_type_ids'],\n",
        "        'start_positions': examples['answer_start'],\n",
        "        'end_positions': examples['answer_end'],\n",
        "        'question_features': input_features['question_features'],\n",
        "        'context_features': input_features['context_features']\n",
        "    }\n",
        "    return output\n",
        "\n",
        "  # Select a random subset of 10000 samples\n",
        "  train_dataset = squad_dataset['train'].shuffle(seed=42).select(range(10000))  # Seed for reproducibility\n",
        "  eval_dataset = squad_dataset['validation'].map(preprocess_squad, batched=True, remove_columns=squad_dataset['validation'].column_names)\n",
        "\n",
        "  return train_dataset, eval_dataset\n",
        "\n",
        "# Training and evaluation functions\n",
        "def train_epoch(model, train_dataloader, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in tqdm(train_dataloader, desc='Training'):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        loss, _, _ = model(**batch)\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    return total_loss / len(train_dataloader)\n",
        "\n",
        "def eval_model(model, eval_dataloader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for batch in eval_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            loss, start_logits, end_logits = model(**batch)\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(eval_dataloader)\n",
        "\n",
        "# Main training loop\n",
        "def train_model(model, train_dataloader, eval_dataloader, optimizer, num_epochs=3):\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(num_epochs):\n",
        "        train_loss = train_epoch(model, train_dataloader, optimizer)\n",
        "        val_loss = eval_model(model, eval_dataloader)\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "\n",
        "# Load datasets and model\n",
        "train_dataset, eval_dataset = load_preprocess_squad()\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "eval_dataloader = torch.utils.data.DataLoader(eval_dataset, batch_size=8)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_config = RobertaConfig.from_pretrained('roberta-base')\n",
        "model = CustomQAModel.from_pretrained('roberta-base', config=model_config)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_dataloader, eval_dataloader, optimizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pXuy1q3ZyHh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUeNF6RmZyFB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVZhoaI3ZyCC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1W4Ua46iZy98"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b1cbd7191d7b4889abc580084a09048a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cde97930cdda4e3c90902a6a88837dd2",
              "IPY_MODEL_d5ab8980a3a349e1bd289cb8c8fcb5ed",
              "IPY_MODEL_45bd97adda5a4b59b6bfaa9d2b591cd8"
            ],
            "layout": "IPY_MODEL_b7e1ec1e577b46a094a5b28779ba4180"
          }
        },
        "cde97930cdda4e3c90902a6a88837dd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b7de68c08d74c81b0fc6959263c7636",
            "placeholder": "​",
            "style": "IPY_MODEL_c2a3b6ad2a6c444199a86eaa6852de24",
            "value": "Map:   0%"
          }
        },
        "d5ab8980a3a349e1bd289cb8c8fcb5ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f8c834adbe24576b20f4cdcf078802f",
            "max": 10570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_349433c0ab0f461083fc88ee5033fb69",
            "value": 0
          }
        },
        "45bd97adda5a4b59b6bfaa9d2b591cd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c832f729bfab4a8280a663109e8b3801",
            "placeholder": "​",
            "style": "IPY_MODEL_2a59635c81664009a06f0b0178c74a30",
            "value": " 0/10570 [00:04&lt;?, ? examples/s]"
          }
        },
        "b7e1ec1e577b46a094a5b28779ba4180": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b7de68c08d74c81b0fc6959263c7636": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c2a3b6ad2a6c444199a86eaa6852de24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f8c834adbe24576b20f4cdcf078802f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "349433c0ab0f461083fc88ee5033fb69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c832f729bfab4a8280a663109e8b3801": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a59635c81664009a06f0b0178c74a30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}